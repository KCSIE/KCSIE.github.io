---
title: Distributed Hash Table - 2 (Kademlia version) - EN
tags: DHT BitTorrent IPFS P2P Blockchain Network
---

# About Kademlia

Kademlia is a distributed hash table designed by Petar Maymounkov and David Mazières in 2002. It was proposed in [*Kademlia: A Peer-to-peer information system based on the XOR Metric*](http://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf). It specifies the structure of the network and the exchange of information through node lookups. Kademlia nodes communicate among themselves using UDP. A virtual or overlay network is formed by the participant nodes. Each node is identified by a number or node ID. The node ID serves not only as identification, but the Kademlia algorithm uses the node ID to locate values (usually file hashes or keywords). In fact, the node ID provides a direct map to file hashes and that node stores information on where to obtain the file or resource.

When searching for some value, the algorithm needs to know the associated key and explores the network in several steps. Each step will find nodes that are closer to the key until the contacted node returns the value or no more closer nodes are found. This is very efficient: like many other DHTs, Kademlia contacts only O(log(n)) nodes during the search out of a total of n nodes in the system.

Kademlia will be abbreviated as Kad in the latter part, while the specific implementation will take Mainline DHT in BitTorrent, a kind of Kad algorithm, as an example.



# Structure

## Keywords

### Node ID

In P2P networks, nodes are identified by a unique ID, which in the original Kad algorithm uses a 160-bit hash space as the Node ID.

Kademlia uses SHA1 hash to calculate Node ID, SHA1 is a 160 bit hash space, the whole ID length is 160 bits, that is 20 bytes, so it can hold 2^160 nodes. IPFS uses SHA256 to calculate Node ID, the ID length is 256 bit hash space, that is 32 bytes. Ethereum uses SHA3, which is also a 256-bit hash space of 32 bytes.

### Node Distance

Each node keeps information about its own neighborhood. In Kademlia, the node-to-node distance is not the physical distance, but the logical distance obtained by the XOR operation.

### XOR

The XOR operation is a bitwise operation that calculates the distance between two nodes. The smaller the result of XOR, the closer the distance is, usually in combination with prefix matching.

XOR means different is true and the same is false, with the symbol ⊕, and has the following characteristics.

- (A ⊕ B) == (B ⊕ A): XOR conforms to the "exchange law" and has symmetry. the distance between A and B is the same from any node, in contrast to Chord's distance algorithm, which is not symmetric.
- (A ⊕ A) == 0: transitivity, distance between self and self is zero.
- (A ⊕ B) > 0: The distance between two different keys must be greater than zero.
- (A ⊕ B) + (B ⊕ C) >= (A ⊕ C): trigonometric inequality, the distance from A to C through B is always greater than the distance from A to C directly.

For example, 0010 ⊕ 0000 = 0010,  0111 ⊕ 0010 = 0101.

### K-Bucket

Use a K-bucket to keep a list of all nodes that are within a certain distance from the current node, bucket0, bucket1, bucket2 ... bucketN record [1, 2), [2, 4), [4, 8), ... [2^i, 2^(i+1)).

### Routing Table

The routing table records all K-buckets, each K-bucket is limited to hold up to k nodes.

## Specific Structure

### Binary Trees

In Kademlia, Kad maps Node IDs (Keys) to a binary tree, and each Node ID (Key) is a leaf of this binary tree.

Mapping rules.

- The Node ID (Key) is represented in binary form, and then processed in order from the high bit to the low bit
- The nth bit of the binary corresponds to the nth level of the binary tree
- If the bit is 1, it goes to the left subtree, and if it is 0, it goes to the right subtree (artificial agreement, can be changed)
- After all the bits are processed, the Node ID corresponds to a leaf in the binary tree

### Distance and XOR

In Kademlia, a binary tree is built based on the number of prefix bits that match the Node ID of the current node and the Node IDs of other peer nodes it keeps, where the number of prefix bits matched is also called LCP (Longest Common Prefix). For example, if the current node is 1101, the LCP is 1 if it does not match from the second bit after the highest bit of the prefix matching with 1000. Each LCP of the current node is a subtree. Obviously, if two nodes share fewer ancestor nodes (with different high bits), they are far apart; conversely, if they share more ancestor nodes (with the same high bits), they are close together.

Suppose the current node ID is 0011, which can be divided into 4 subtrees with LCP = 3(none/), 2(0/), 1(00/), 0(001/).

[![RXD7Zt.md.png](https://z3.ax1x.com/2021/07/08/RXD7Zt.md.png)](https://imgtu.com/i/RXD7Zt)

For any given node, we divide the binary tree from the root node continuously down into a series of subtrees that do not contain that node. The highest subtree consists of half of the binary tree that does not contain that node, the next subtree consists of the remaining half of the tree that does not contain that node. If the height of this binary tree is m + 1, we will get m subtrees.

### K-Bucket and Routing Table

Each subtree is a K bucket, for a 160 bit space Node ID there are 160 subtrees in total i.e. 160 K buckets, (parameter Keyspace = 160 bit). Each K-bucket can be indexed by the result of XOR. The number of leaf nodes contained in a subtree is set to a maximum of k, k is the number of nodes that can be stored in each K-bucket (Usually, k = 20. It's 8 in Mainline DHT).

[![RXfT10.png](https://z3.ax1x.com/2021/07/08/RXfT10.png)](https://imgtu.com/i/RXfT10)

K nodes are taken in each subtree to form m K-buckets, and these m K-buckets are the routing table. We define the node obtained in the smallest subtree as the 0th k-bucket, the node obtained in the next smallest subtree as the 1st k-bucket, and so on. For each 0⩽i<m, the distance between the node in the i-th k-bucket and the current node is always within the interval [2^i, 2^(i+1)). Taking the K-bucket of node 101 at m = 3, k = 2 as an example：

[![RX6vt0.md.png](https://z3.ax1x.com/2021/07/08/RX6vt0.md.png)](https://imgtu.com/i/RX6vt0)

The following figure is another example:

[![RXRHCq.png](https://z3.ax1x.com/2021/07/08/RXRHCq.png)](https://imgtu.com/i/RXRHCq)

For Node 0010, the Node 0000 procedure is as follows:

- Distance = 0010 ⊕ 0000 = 0010 = 2
- The distance is in the interval [2^i, 2^(i+1)) (i = 1), i.e. 2 to 3, so Node 0000 should be placed in K-bucket1

It can also be determined by LCP and the highest non-zero bit of the result of XOR:

- 0011 and 0010, the first 3 bits match, the highest non-zero bit of XOR result is 0, put it in bucket 0
- 0000/0001 and 0010, the first 2 bits match, the highest non-zero bit of XOR result is 1, put it in bucket 1
- 0100/0110/0101/0111 and 0010, the first 1 bits match, the highest non-zero bit of XOR result is 0, put it in bucket 2



# Algorithm

## Message

The Kademlia protocol consists of four RPC calls: PING, STORE, FIND_NODE, FIND_VALUE.

- PING message --- used to test if the node is still online
- STORE message --- to store a key-value pair in a node
- FIND_NODE message --- the recipient of the message request will return the k nodes which are closest to the requested key value in its bucket
- FIND_VALUE message --- Same as FIND_NODE, but when the recipient of the request has the key requested by the requester, it will return the value of the corresponding key.

Each RPC message contains a random value added by the initiator, which ensures that the response message is received in a way that matches the request message sent earlier and prevents address forgery.

A more specific implementation can be found in the KRPC protocol in Bittorrent's Mainline DHT, which is a simple RPC structure consisting of bencode encoding, using UDP. It contains 3 message types: request/reply/error and four types of requests.

- PING --- used to test if the node is still online, to assist in routing table updates
- ANNOUNCE_PEER(STORE) --- Notify other nodes to start downloading a resource themselves, `announce_peer` carries the token in the `get_peer` response message
- FIND_NODE --- used to find a node, to get its address information
- GET_PEERS(FIND_VALUE) --- a list of peers corresponding to the resource, obtained through the resource's infohash

## Look-up

The most important process that a node needs to implement in Kademlia is to find the k nodes closest to it based on a node ID, this process is the node lookup, which is mainly done using recursion.

As mentioned before, the basic operation related to the node storage process is the `FIND_NODE` operation, which accepts a Key as an argument and returns the k nearest nodes to the Key that the current node knows. The process of finding the k nearest nodes based on K buckets is as follows: first find the distance d between the Key and the current node; the distance between the node in the i-th K bucket and the current node is in the interval [2^i, 2^(i+1)), these intervals do not overlap each other, and the node in the K bucket where d falls in the interval is the nearest node to the Key. If there are less than k nodes in this K-bucket, the node is taken in the next K-bucket to replenish it, and if it is still not enough, take it in the next K-bucket again. If the sum of all the nodes in this node's K buckets is less than k, it returns all the nodes it knows.

The node lookup is then performed. Finding the nearest k nodes in the entire network given a Key is a recursive process: the starting node needed for the lookup first takes α nodes from the non-empty K bucket closest to the target node Node ID (if there are less than α nodes in the K bucket, the full α nodes are taken from the other K buckets). The whole process is that the initial node calls its own `FIND_NODE` and finds k nodes that it knows are closest to the Key. Next, we take α nearest nodes among these k nodes and request them to perform `FIND_NODE` for the Key (α is a system-level constant to set the number of simultaneous requests).

## Routing Table Maintenance

### Binary Tree Splitting

For each node, the entire binary tree can be split into up to 160 subtrees according to its own perspective. The rule of splitting is: start from the root node and split the subtree that does not contain itself; then split the second subtree that does not contain itself in the remaining subtrees; and so on, until finally only itself remains. For each node, when it finishes splitting the subtree with its own view, it will get n subtrees; for each subtree, if it knows a node inside, then it can use these n nodes for recursive routing to reach any node in the whole binary tree.

The prefix tree (Tire) is used in the Mainline DHT, but is slightly different. When inserting a key in a Trie tree, we have to compare each char of the key with the path in the Trie, and when it does not match, it will split into a subtree immediately. However, in Mainline DHT, when there is inconsistency, there is no immediate splitting, but a bucket of length k. Two cases are discussed：

- If the bucket length is less than k, just insert the bucket directly
- If the bucket length is greater than or equal to k, discuss it in another two cases:
  - The first case is that the current path is the prefix of the node Node ID (note that it is not the key to be inserted, but its own Node ID), then split, the key of the left and right subtrees are 0 and 1, and the nodes in the current bucket are divided into the bucket of the corresponding subtree according to their current char value
  - The second case is that the current path is not a prefix of the Node ID of the node, in this case, the key is dropped directly

### Routing Table's Change

The routing table is a binary tree with leaf nodes as K-buckets, and the set of all K-buckets covers the entire 160 bit space without overlap. The nodes in the routing table are dynamically allocated on demand. In the initial state, node u has only one node in the routing table, and one K-bucket covers the entire Node ID space. When u gets a new neighboring node, it will try to insert it into the appropriate K bucket based on the distance to the new neighboring node. The same as the subtree splitting above, the rules for insertion are as follows:

- If the K bucket is not full, the new neighboring node will be inserted directly
- If the K bucket is full.
  - If the range of the K bucket includes u's own Node ID, the K bucket will be split in two and the Node ID in the original K bucket will be reassigned to the new K bucket and then the insertion will be attempted again
  - If the range of the K bucket does not include u's own Node ID, the new neighboring node will be discarded directly
    - Problems can arise in highly unbalanced binary trees, assuming u is the only node with a Node ID starting with 000 and there are more than k nodes with IDs starting with 001 v1,v2...vk...vn. For each node v's routing table, u will be For each node v, u is inserted into an empty k-bucket. u's bucket update is only notified to k of all v's recorded in u's routing table
    - To avoid that the remaining v's do not get u's bucket update information, the nodes in the Kad are additionally split to save all valid nodes over k, even if not because the K bucket includes its own Node ID

[![RzZeat.md.png](https://z3.ax1x.com/2021/07/09/RzZeat.md.png)](https://imgtu.com/i/RzZeat)

## K-Bucket  Maintenance

### Refresh K-Bucket

When a node receives any request or reply message from another node, it updates the K bucket according to the sender's node ID. The rule is as follows.

- The nodes in each bucket are sorted in reverse order of the last contact time

- If the sending node already exists in the receiver's K bucket
  - The receiver moves the sender node to the end of the corresponding K bucket
- If the sending node is not in the receiver's K bucket
  - If the number of nodes in the K bucket is less than k, the sending node is inserted directly to the end of the K bucket
  - If the K bucket is full
    - If the least recent active node cannot ping through, remove the node and insert the sender node to the end of the K bucket
    - If the least active node can ping through, move the node to the tail and discard the sending node

The K bucket follows the Least Recently Used (LRU) elimination algorithm, where the least recently active nodes are ranked at the head of the K bucket and the most recently active nodes are ranked at the tail of the K bucket, which ensures that any node joining or leaving does not affect the overall network.

### Join the Network

In order to join the network, a node u has to be associated with a node w that already exists in the network. u inserts w into the appropriate K bucket, then u performs a node lookup on its own Node ID, and finally u contacts a node further away from its neighbors and updates the corresponding K bucket. During the refresh process, u constructs its own K bucket and also inserts itself into the K buckets of other nodes.

When a node joins, in addition to constructing a K bucket, it should also fetch the resources for which this node is responsible. at regular intervals in Kad, all nodes perform a post operation on the resources they own; in addition at regular intervals nodes discard resources that have not received a post message during this time. In this way, new nodes receive the resources they are responsible for, while the resources are always kept in the hands of the k nodes closest to them. Node exiting in Kad is not like the Chord algorithm, where nodes can exit without doing anything due to the k-bucket refresh and resource reissue mechanism.

------

**Reference/参考**: 

1. [Distributed hash table](https://en.wikipedia.org/wiki/Distributed_hash_table)
2. [DHT 网络之 Kademlia 算法](https://0ranga.com/2018/11/08/dht-kademlia/)
3. [DHT 分布式哈希表](https://colobu.com/2018/03/26/distributed-hash-table/)
4. [易懂分布式 Kademlia算法](https://www.jianshu.com/p/f2c31e632f1d)
5. [P2P 网络核心技术：Kademlia 协议](https://zhuanlan.zhihu.com/p/40286711)
6. [DHT Protocol：BitTorrent DHT 协议中文翻译](https://justjavac.com/other/2015/02/01/bittorrent-dht-protocol.html)



